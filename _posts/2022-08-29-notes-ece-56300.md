---
layout: post
title: "Notes: ECE 56300 - Programming Parallel Machines"
date: 2022-09-05
excerpt: "Study notes / 2022 Fall, Purdue, ECE"
tags: [parallel programming, threads, multi-process, openmp, mapreduce, cuda]
feature: /assets/img/bg.jpeg
---

<style>
.highlight {
    line-height: 1;
    margin-bottom: 0;

}

.highlight::-webkit-scrollbar {
    width: 0 !important;
}

li {
    text-align: left;
}

li code {
    line-height: 1;
}
</style>


> **Table of Contents**
> * this unordered seed list will be replaced by toc as unordered list
> {:toc}

<br/>


<!-- LECTURE 1 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 1

- **Dependences: Orderings that must be enforced to ensure that correct values are read**
  - The dependencies are the thing that keeps us from being able to completely parallelize our program<br/>
     (dependence = the different iterations are dependent on one another)
  - **<span style="color:blue">Cross iteration / Loop carried:</span>** dependences that go from one iteration to a later iteration 
    ~~~ c
    
    /*  Example: true dependence  */
    for (i=1 ; i<n ; i++) {
      a[i] = b[i] + c[i];   // write to  a[i]   in iteration i
      c[i] = a[i-1];        // read from a[i-1] in iteration i
                            // a[i-1] cannot be overwritten by the thread that executes i-1 iteration
    }
    
    ~~~
    - This dependence is **flow, or true, dependence**
    - It is fundamental to the algorithm that follows the flow of data **from a write to a read**
  - **<span style="color:blue">Anti-dependence:</span>** when data must be read before it is overwritten
    ~~~ c

    /*  Example: anti-dependence  */
    for (i=1 ; i<n ; i++) {
      a[i] = b[i] + c[i];   // write to  a[i]   in iteration i
      c[i] = a[i+1];        // read from a[i+1] in iteration i
                            // a[i+1] cannot be overwritten by the thread that executes i+1 iteration
    }

    ~~~
    - It is NOT fundamental to the algorithm, since **the data can be copied at first**
    - It is a rise in the need to reuse storage, so it is an efficiency consideration
  - **<span style="color:blue">Output dependence:</span>** when a write must finish before another write to the same location
    ~~~ c

    /*  Example: output dependence  */
    for (i=1 ; i<n ; i++) {
      a[i] = b[i] + c[i];   // write to  a[i]   in iteration i
      a[i-1] = a[i];        // read from a[i-1] in iteration i
                            // a[i] cannot be overwritten by the thread that executes i+1 iteration
    }

    ~~~
  - In hardware, these are **WR, RW and WW hazards**
  - **Parallelizable:** if  dependences do not cross sections of code that run in parallel<br/><br/>

- **<span style="color:blue">Amdahl's law</span>**
  - If we parallelize 90% of a program we will get, at most, a speedup of 10X, 99% a speedup of 100X <br/>
    To effectively utilize 1000s of processors, we need to parallelize 99.9% or more of a program
  - **Optimistic since it ignores the IO and communication costs**<br/><br/>

- **Short architectural overview**
  - Floating point unit, Floating point registers
  - Arithmetic logic unit (ALU), General purpose registers
  - Program counter, Instruction decode unit
  - **<span style="color:blue">L1 cache (smallest and fastest),</span>** L2 cache, L3 cache, Memory controller
  - **<span style="color:blue">Registers</span>**
    - Usually directly referenced and accessed by **machine code instructions**
    - **Architected registers:** visible to programmers to manipulate and make instructions
    - **Cache is not architected**
  - **<span style="color:blue">Caches:</span>** when you read one word, **several words are brought into cache**
    - **Tiling** solves the problem of **accessing arrays within the same cache line** to reduce overhead
    - If we are not able to do the access in an order that allows us to effectively use the caches, then let's see if we can **break our program up into chunks that fit with the cache line**
    - Tiling reduces the data brought from memory by a **factor of N since tiling increases locality,** which is the property that operations in successive time steps operate on data that area close

<br/>


<!-- LECTURE 2 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 2

- **Processor**
  - Components
    1. Fetch and decode instruction
    2. **ALUs**
    3. Vector units
    4. Programmable execution context (registers, condition codes, etc.)
    5. Processor-managed execution context (cache, various buffers, translation tables, etc.)
  - **<span style="color:blue">Instruction level parallelism (ILP)</span>**
    - Processors figure out that multiple instructions that can be executed at the same time
    - **Average number of instructions per cycle is less than 2,** depending on the application and architecture<br/><br/>

- **<span style="color:blue">Multiprocessor = Shared memory multiprocessor</span>**
  - Multiple CPUs with a shared memory, or **multicores** (a single chip multiprocessor with multiple cores)
  - Two variants:
    - **Uniform memory access:** all processors access all DRAM memory in the same amount of time (e.g., multicores)
    - Non-uniform memory access: different processors need different amounts of time to access memroy location
  - Coherence is important with multiprocessors due to the **private cache of each processor**
  - **<span style="color:blue">Invalidates</span> are executed across different cores** when a value is loaded into a cache that can be written<br/>
    (i.e., that may change the value of variables stored in another cores or processors cache)
  - **Responsibilities:**
    - The **hardware** ensures that the last values stored to a physical address are the ones read by the next read
    - The **software** must ensure that the order of loads and stores is consistent
  - **<span style="color:blue">Sequential Consistency (SC)</span>**<br/>
    > **Sequential** consistency is when the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.
    - **Coherence** says that a read gets the last value written for a variable in a shared memory multiprocessor
    - **Consistency** is concerned with the interactions between reads an writes to different variables

<br/>


<!-- LECTURE 3 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 3

- **Terminology**
  - **<span style="color:blue">Processors:</span>** a physical piece of hardware **with one ore more cores** that executes instructions
  - **<span style="color:blue">Nodes:</span>** contain one or more processors, DRAM, and network cards to communicate with other nodes
  - **<span style="color:blue">Processes</span>**
    - The name for a single job that has a single virtualized image of the system is a **process**
    - OS allows multiple jobs and users to access to machine at the same time by **virtualizing the machine**
    - **Time-sharing / multiplexing of processes** allows jobs running simultaneously, even with a single core
    - This allows the OS to **<span style="color:blue">protect jobs from one another &rarr; virtualization of memory</span>**
    - Virtual memory allows each job to act as if it has access to the entire address space of memory
    - Communication among processes: **<span style="color:blue">sockets, MPI (Message Passing Interface)</span>**
  - **<span style="color:blue">Threads</span>**
    - Allow multiple instruction streams to communicate by reading and writing to the **same memory addresses**
    - Threads are usually **<span style="color:blue">managed by the OS, but a given thread is owned by a process</span>**
    - Threads owned by the process **share the virtualized resources owned by the process** and **run on a core**
  <br/><br/>

- Parallelism
  - **Thread-level**
    - Typically within a node or a single processor machine &rarr; **OpenMP and Pthreads**
    - GPUs use a form of thread parallelism, and there are also systems that support thread parallelism across nodes
  - **Process-level**
    - Parallelism is across processes and typically across nodes &rarr; **MPI**

<br/>


<!-- LECTURE 4 - 5 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 4 - 5

- **<span style="color:blue">OpenMP</span>**
  - Full name: Open Multi-Processor &rarr; target multicores and multi-processor shared memory machines
  - Implemented using **compiler directives (pragmas)** and library support
  - **Can be used with MPI** to allow both node/process and thread level parallelism
  - Hardward model: **uniform memory access** machine is assumed
  - **<span style="color:blue">Fork/Join parallelism</span>**
    - Program execution starts with a single master thread
    - When parallel part of the program is encountered, **a fork utilizes other worker threads**
    - **The overheads** come from waking up or creating threads, then spreading for the parallel execution
    - At the end of the parallel region, **a join kills or suspends the worker threads**
  - **Data parallel operations:** the same operation applied to many independent data elements
  - **<span style="color:blue">Execution context</span>**
    - **Shared:** variables on the stack belonging to functions that invoke the threads
    - **Private:** a thread-local stack for functions that invoke and block the threads
  - **<span style="color:blue">Hyperthreading</span>**
    - By **replicating the programmable execution context,** threads can be scheduled and run on a same core
    - **Non-beneficial for numerical programs** due to the competition for computational resources and cache
    <br/><br/>

  - Running threads more than the cores might decrease the performance due to **the conflicts on cache** between threads
  - No guarantee that a thread gets scheduled back onto same core when taking off from a core for other threads to come in
  - **Decision algorithms** are needed when there are empty cores but OSs attempt to schedule a thread onto the same core
  <br/><br/>

- **Basic usage**
  ~~~ c

  /*
      #pragma omp parallel for
      for (index=START ; index REL-OP val ; incr)
  */

  int i, j;
  #pragma omp parallel for private(j), shared(t)
  for (i=0 ; i<n ; i++) {                       // private as index variable
      for (j=0 ; j<n ; j++) {                   // private as private(j)
          int t;                                // shared  as shared(t)
          a[i][j] = max(b[i][j], a[i][j]);
      }
  }


  int t = 0;
  #pragma omp parallel for firstprivate(t), lastprivate(t)
  for (i=0 ; i<n ; i++) {                       // private                      as index variable
      t = t + a[i];                             // private and intialized to 0  as firstprivate(t)
  }
  t = t / n;                                    // t is from iteration i = n-1  as lastprivate(t)
  /*  BUT the above solution is WRONG, t is not really the total summation  */
  
  ~~~

  ~~~ c

  /*  Atomicity (Locks)
      - the following solution would be WRONG if without the atomic operations
        since some updates regarding accumulating t would be missing due to races
      - the following solution is CORRECT but "critical" SLOW down the execution
  */
  int t = 0;
  #pragma omp parallel for
  for (i=0 ; i<n ; i++) {                       // private
      #pragma omp critical
      t = t + a[i];                             // shared
  }
  t = t / n;

  ~~~

  - OpenMP needs enough information from **for loop** to divide the iterations among the different threads
    - OpenMP **automatically make index variables and variables inside the parallel region private** for each thread
  - For **nested for loops**, prefer **parallelizing the outer loop** because fewer fork-joins
  - **Locks** do not enforce ordering
  <br/><br/>

- **<span style="color:blue">Reduction</span>**
  ~~~ c

  /*  Reduction
      - Make t private
        Put the partial sums for each thread into t
        Form the full sum of t
      
      Operations on the reduction variable
      - x = x OP expr           x must be scalar, expr cannot reference x
      - x = expr OP x           except subtraction
      - x = func(x, expr)       func is MIN or MAX
      - x++, x--, ++x, --x
  */
  int i;
  int t = 0;
  // Parallel loop startup and teardown has a cost
  // Parallel loops with relatively few iterations can lead to slowdowns
  #pragma omp parallel for reduction(+:t) if (n>10000)
  for (i=0; i<n; i++) {
      t = t + a[i];
  }
  t = t/n;

  ~~~
  - Take something with d dimensions and **reduce it to something with d-k dimensions**
  - Reductions on **commutative operations** can be performed in parallel
  <br/><br/>

- **<span style="color:blue">Thread Scheduling</span>**
  ~~~ c

  #pragma omp parallel for schedule(static) nowait
  for (i=0; i < n; i++) {
    if (a[i] > 0) a[i] += b[i];
  }
  
  // no barrier here with nowait
  
  #pragma omp parallel for schedule(static)
  for (i=0; i < n; i++) {
    if (a[i] < 0) b[i] = -a[i];
  }

  ~~~

  - The schedule clause allows us to guide **how iterations of a loop are assigned to threads**
  - The unit of scheduled iterations is **<span style="color:blue">chunk</span>, a contiguous set of iterations**
  - Schedule can be set by an environment variable ```setenv OMP_SCHEDULE="static,1"```
  - **<span style="color:blue">Static:</span>** iterations are assigned to threads at the start of the loop
    - Low overhead, but possible **load imbalance** issues
    - The same thread will execute the **same iteration in two different loops** if
      1. The bounds of two loops are the same
      2. They are both scueduled statically
      3. They both have the same chunk size
    - If so, the **nowait** clause can help remove the barrier between two loops
  - **<span style="color:blue">Dynamic:</span>** some iterations are assigned as loop execution progresses
    - **Higher overhead,** but better load balance
  - **<span style="color:blue">Guided:</span>** **Large chunks are initially distributed,** with smaller chunks distributed as loop executes
    - **Large blocks** <u>reduce the scheduling overhead</u> (few blocks to schedule) but can lead to larger load imbalances
    - **Smaller blocks** <u>increase the scheduling overhead</u> (more blocks to schedule) but with smaller load imbalances
  <br/><br/>

- **Parallel Blocks**
  ~~~ c

  #pragma omp parallel sections
  {
    #pragma omp section   // optional
    {
      v=f1( );
      w=f2( );
    }
    
    #pragma omp section   // not optional
    v=f3( );
  }

  ~~~

  ~~~ c

  #pragma omp parallel private(w)
  {
    w = getWork( );
    while (w != nullptr) {
      doWork(w);
      w = getWork( );
    }

    #pragma omp single
    printf("finishing work");
  }

  ~~~

<br/>


<!-- LECTURE 6 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 6

- **<span style="color:blue">Locks</span>**
  - **OpenMP threading is built on top of pthread** so the lock operations are the similar
  - Make sure that the same element is protected by the same locks, also, **the total number of locks is limited**
  <br/><br/>

- **<span style="color:blue">OpenMP Memory Model</span>**
  - **Coherence:** Behavior of the memory system when a single address is accessed by multiple threads
    - <u>Hardware dependences (hazards)</u> are used to deal with reads / writes within a thread to the same memory location
  - **Consistency:** Orderings of accesses to different addresses by multiple threads
    - <u>Memory models</u> worry about the interactions of loads / stores (reads / writes) in different threads
  - **Compiler** transforms the source code into executable code in **semantically equivalent single thread order**
  <br/><br/>

- **Synchronization**
  - **Use synchronization to constrain the reorders** that compilers and hardware can put on our program
  - Thus we will end up with **sequential consistency** executions
  - **Cross thread orderings** come from synchronization
    > **Race:** if there is R<sub>read</sub> or W<sub>write</sub> to V<sub>variable</sub> in T<sub>thread</sub> and W<sub>write</sub> to V<sub>variable</sub> in other thread T'<sub>thread</sub> and there is no enforced ordering at runtime between the two threads, there is a race.
  <br/>
  - **<span style="color:blue">Sequential Consistency (SC)</span>**
    - An operation is SC <u>if the operation is in the same order in the program order, code order and commit order</u>
    - Most of languages and processors have **relaxed consistency model** (i.e., some of these orders can be violated)
    - Consistency models are based on orderings of **Reads (R), Writes (W) and Synchronizations (S)** within a thread
  - **OpenMP consistency models**
    - **<span style="color:blue">Weak consistency,</span>** not sequential consistent
    - **Synchronization operations (S)** must be executed in sequential order
    - Within a thread
      1. OpenMP will not not reorder reads or writes past a synchronization
      2. Will not reorder an S operation with respect to a W operation
      3. Will not reorder an S operation with respect to a R operation
      4. **Guarantee <code>S &rarr; W</code>, <code>S &rarr; R</code>, <code>R &rarr; S</code>, <code>W &rarr; S</code>, <code>S &rarr; S</code>**
      5. Not guarantee <code>R &rarr; R</code>, <code>R &rarr; W</code>, <code>W &rarr; R</code>, <code>W &rarr; R</code>
  - **```flush```** all thread visible variables and **force a consistent view between the local and shared memory**
    - **Locks** always execute a flush, as do **barriers**

    > Considering a race condition that two threads do 64-bit write using MAX and ZERO respectively to the same variable at the same time, for a 32-bit processor, **the final value of that variable has four possiblities.**
  
<br/>


<!-- LECTURE 7 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 7

- **OpenMP (before 3.0)**
  - A parallel construct creates **implicit tasks** (one per thread)
  - A team of threads are createrd to execute the tasks
  - Each thread is assigned and tied to one task
- **<span style="color:blue">OpenMP 3.0 allows us to explicitly create tasks</span>**
  - Syntax: **```#pragma omp task [clause[[,] clause] ...]```**
  - Clauses: **if (expression), untied,** shared, private, firstprivate, ...
  - **<span style="color:blue">```if (false)```</span>** says execute the task using the spawning thread **(data environment is local to the spawning thread)**
  - **<span style="color:blue">```untied```</span>** says the **task can be executed by more than one thread at different points in time**

  ~~~ c

  /*  Example: Post-order tree traversal  */

  void postorder(node *p) {
      if (p->left)
          #pragma omp task
          postorder(p->left);
      
      if (p->right)
          #pragma omp task
          postorder(p->right);
      
      //  Task scheduling point
      #pragma omp taskwait      // the parent node waits for descendants
      process(p->data);
  }

  #pragma omp parallel
  {
      //  using one thred to kick off traversing to avoid doing redundant tasks
      //  #pragma single or #pragma master
      #pragma single
      postorder(root);
  }

  ~~~
  
- **<span style="color:blue">Task scheduling points</span>**
  - Threads at task scheduling points can **suspend their task and begin executing another task in the task pool**
  - Which thread picks up the tasks depends on **```tied``` or ```untied```**
  - At the completion of the task, **it can resume executing the original task or picking up the next task** in the pool
  - **```taskprivate```** data is supported

  ~~~ c

  /*  Task switching
      - the thread generating tasks can
        1. execute an already generated task, draining the task pool
        2. execute the encountered task (could be cache friendly)    */
  #pragma omp single
  {
      for (i=0; i<ONEZILLION; i++)
          #pragma omp task    // tied
          process(item[i]);
  }


  /*  Thread switching
      - The task that generates tasks can be resumed by a different thread and generate tasks   
      - The cost would be the cache issue when different threads generate tasks   */
  #pragma omp single
  {
      #pragma omp task untied
      for (i=0; i<ONEZILLION; i++)
          #pragma omp task    // tied
          process(item[i]);
  }

  ~~~

- **Runtime Library Calls:**
  ~~~ c

  /*  System-related  */
  omp_num_procs()
  omp_in_parallel()
  omp_thread_limit
  omp_get_thread_limit()
  omp_get_max_threads()
  double omp_get_wtime()
  double omp_get_wtick()

  /*  Scheduling  */
  omp_set_dynamic(bool)     // default: true
  omp_get_dynamic()
  omp_set_schedule()
  omp_get_schedule()
  omp_set_schedule(omp_sched_static, 5);
  
  
  /*  Nested parallelism  */
  omp_set_num_threads()     // within a parallel construct sets the number of threads available to the next level
  omp_get_active_level()
  omp_get_ancestor(level)
  omp_get_teamsize(level)
  omp_set_max_active_levels()
  omp_get_max_active_levels()
  OMP_MAX_ACTIVE_LEVELS     // environment variable
  ~~~
  
  - **Portability**
    - Environment variable to control stack size added: **```omp_stacksize```**
    - Environment variable to specify how to handle idle threads: **```omp_wait_policy```**
      1. **ACTIVE:** keep threads alive at barriers/locks
      2. **PASSIVE:** try to release threads to the processor


<br/>


<!-- LECTURE 8 - 9 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 8 - 9

- **<span style="color:blue">Message Passing Interface (MPI)</span>**
  - **Distributed memory system across different machines,** instead of shared memory model in a single node
  - All communication between processes are **explicitly specified** by the programmer
  - Fine grained communication is too expensive, in general, aggregating communication has a better performance
  - Across processors, some can be reached in a single hop on the network, while others require multiple hops
  - **Why using MPI?**
    - Allows use of much larger machines than with shared memory
    - **Allows control over data layout, locality, and communication** – very important on large machines
    - **Universal parallel programming model** – portable across all machines including shared memory machine
  - **Why not using MPI?**
    - Some programs naturally fit on shared memory
    - Information needed for messages low-level and sometimes hard to program
    - **Message passing code disrupts the flow of algorithms**
  - **<span style="color:blue">Single Program Multiple Data (SPMD)</span>**
    - Multiple copies of the same program execute on **different parts of the data**
    - Programs in different processes can execute in **different paths through the program** (e.g., if and else)
    - **Need to perform communication** if any data dependence across different nodes due to the lack of shared memory
    - **Each process has a unique rank or process id** (often called pid in programs)
    - It is not changed during the execution of the program
    - **Data is often spread across multiple files to accommodate parallel I/O** on large problems
  - **<span style="color:blue">Data management</span>**
    1. **<code>r = N<sub>#arrayElements</sub> mod P<sub>#processes</sub></code>**
      - if ```r = 0``` all blocks are the same size with ```N/P``` elements
      - **Otherwise the first ```r``` blocks have ```ceil(N/P)``` and the last ```P - r``` have ```floor(N/P)``` elements**
      - First element on a process is ```p * floor(N/P) + min(p, r)```
      - Last element on a process is ```(p + 1) * floor(N/P) + min(p + 1, r) - 1```
      - Process with element<sub>i</sub> is ```min(ceil(i / (ceil(N/P) + 1)), i - r) / floor(N/P)```
    2. **<code>N<sub>#arrayElements</sub> div P<sub>#processes</sub></code>**
      - First element on a process is ```floor(p * N/P)```
      - Last element on a process is ```floor((p + 1) * N/P) - 1```
      - Process with element<sub>i</sub> is ```floor((P * (i + 1) - 1) / N)```
    3. **Cyclic distribution:** this is analogous to (static, 1) scheduling in OpenMP
    4. **Block-cyclic distribution:** this is analogous to (static, C) scheduling in OpenMP
        ~~~ text 

        12 elements over 5 processors

        (1)                         (2)                         (3)
              p0  p1  p2  p3  p4          p0  p1  p2  p3  p4          p0  p1  p2  p3  p4
        ====================================================================================
               0   3   6   8  10           0   2   4   7   9           0   1   2   3   4
               1   4   7   9  11           1   3   5   8  10           5   6   7   8   9
               2   5                               6      11          10  11
              
        ~~~
        <br/>

- **Example (1): <span style="color:blue">Radix sort</span>**
  - Radix sort works well to sort lists of densely distributed numbers  
  - Assume integers have value from 0 to 65,535
  - Have ```N >> 65,535``` numbers to sort
  - All declared variables exist within each process
  - **Data management**
    - There is a global and a local logical index space for arrays
    - The concatenation of the local partitions of data arrays forms the global data array

  ~~~ c

  /*  
      MPI_Reduce(void* opnd,
                  void* result,
                  int count,
                  MPI_Datatype,
                  MPI_Operator op,
                  int root,
                  MPI_Comm comm);
      - opnd:         address of the first element to be reduced
      - result:       address of the first result element
      - count:        number of elements being reduce and number of results
      - MPI_Datatype: type of the element being reduced
      - MPI_Operator: kind of reduction
      - root:         rank of the process getting the result
      - MPI_Comm:     communicator over which the reduction takes place
  */

  // sequential version
  for (i=0 ; i<65535 ; i++) { sorted[i] = 0; }
  for (i=0 ; i<n     ; i++) { sorted[data[i]]++; }
  for (i=0 ; i<65535 ; i++) {
      for (j=0 ; j<sort[i] ; j++) {
          fprint("%i\n", i);
      }
  }

  // parallel version
  #include <mpi.h>
  #include <stdio.h>

  void sort (int sorted[], int data[], int pid, int numP) {
      for (i=0 ; i<BS(pid,numP,N) ; i++) sorted[data[i]]++;
      
      if (pid == 0) {
          MPI_Reduce(MPI_IN_PLACE,    sorted, 65535, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
      } else {
          MPI_Reduce(sorted, (void*) nullptr, 65535, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
      }
          
      if (pid == 0) {
          for (i=0 ; i<65535 ; i++) {
              for (j=0; j < sort[i]; j++) fprint("%i\n", i);
          }
      }
  }

  int main(int argc, char* argv[]) {
      int pid;
      int numP;
      int N;
      extractArgv(&N, argv);

      int sorted[65536];
      int data[N/4];
      MPI_Init(&argc, &argv);
      MPI_Comm_size(MPI_COMM_WORLD, &numP);
      MPI_Comm_rank(MPI_COMM_WORLD, &pid);
      
      for (i=0 ; i<65535 ; i++) sorted[i] = 0;
      MPI_Barrier( );
      
      elapsed = -MPI_Wtime();
      sort(sorted, data, pid, numP);
      elapsed += MPI_Wtime();
      elapsed = elapsed / MPI_Wtick();

      MPI_Finalize();
  }

  ~~~

<br/>


<!-- LECTURE 10 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 10

- **Example (2): <span style="color:blue">Sieve of Erosthenes (count prime numbers)</span>**
  - **Algorithm**
    ~~~ python

    """ Finding prime numbers using Seive """
    # (1) Start with 2 and mark all multiples
    # (2) Find the next unmarked number "u" which is a prime
    # (3) Mark all multiples of "u" between "u^2" and "n" until "k^2" > n
    # (4) Repeat (2) and (3) until finished

    ~~~
  - **<span style="color:blue">Domain decomposition:</span>** how do we break up the data being operated on across processes
    - How we **decompose the program or the data** is directly going to impact the parallelization process
    - Then after we break the data, we are going to **associate the processes with the data**
  - In general, we find the **<span style="color:blue">fundamental operations</span>** of the algorithm
    ~~~ python

    """ Fundamental operations """
    # (1) Marking of the multiples of the last prime found
    # (2) If v is a multiple of k, then v mod k == 0
    # (3) A min-reduction to find the smallest prime k across all processes
    # (4) Broadcast the value of k to all processes
    # (5) Go to 1 and repeat until k^2 > n

    ~~~
    - Put as much work as possible onto a single process as **doing reduction and broadcast take ```logP``` steps**
    - Make the work done by each process similar, i.e., having **load balance**
    - Make the **communication between tasks efficient**
  - **<span style="color:blue">Combining work / Partitioning data (owners compute rule</span>)**
    - Each element is owned by a process – the process that has the consistent and up-to-date value of the data
    - All updates to the data are made by the owner
    - All requests for the value of the variable are to the owner
  - **Data management**
    - Although cyclic disbribution usually gives better load balance, it does not in this case
    - This is true even through primes and work are unevenly distributed across the data space
    - For example, if there are two processes, one will have all odd elements, and one will have all even ones. All the work after the first step will be done on odd elements
    - **A block distribution** gives a better load balance although **the computation of indices will be harder**
  - **Interplay of decomposition and implementation**
    - Let **```L```** be the largest prime number, **```P```** be the number of processes, and **```N```** be the number of elements
    - At most, **<span style="color:blue">only the first ```sqrt(L)``` will be used to mark off (sieve) other primes</span>**
    - If **```N/P > sqrt(L)```** then only elements in **<code>p<sub>pid</sub> = 0</code>** will be used to sieve
    - This means we only need to look fo the lowest unmarked element in process 0, saving a reduction on each step
    - Mark the multiples by **```mod``` operation is expensive, thus use striding and marking things** instead

    ~~~ c

    #include <mpi.h>
    #include <math.h>
    #include <stdio.h>
    #include "MyMPI.h"
    #define MIN(a,b) ((a)<(b)?(a):(b))
    int main (int argc, char *argv[])
    {
        ...

        MPI_Init(&argc, &argv);
        MPI_Barrier(MPI_COMM_WORLD);
        MPI_Comm_rank (MPI_COMM_WORLD, &id);
        MPI_Comm_size (MPI_COMM_WORLD, &p);
        elapsed_time = -MPI_Wtime();

        if (argc != 2) {
            if (!id) printf("Command line: %s <m>\n", argv[0]);
            MPI_Finalize(); exit(1);
        }
        n = atoi(argv[1]);
        low_value  = 2 + BLOCK_LOW( id, p, n-1);
        high_value = 2 + BLOCK_HIGH(id, p, n-1);
        size = BLOCK_SIZE(id, p, n-1);

        /*  Check if cannot find next prime number on process 0  */
        proc0_size = (n-1) / p;
        if ((2 + proc0_size) < (int) sqrt((double) n)) {
            if (!id) printf ("Too many processes\n");
            MPI_Finalize();
            exit (1);
        }

        marked = (char *) malloc (size);
        if (marked == NULL) {
            printf ("Cannot allocate enough memory\n");
            MPI_Finalize();
            exit (1);
        }
        for (i=0 ; i<size ; i++) marked[i] = 0;
        
        /*  pid=0: find next prime  */
        if (!id) index = 0;
        prime = 2;
        do {
            /*  find first value to mark  */
            if (prime * prime > low_value)
                first = prime * prime - low_value;
            else {
                if (!(low_value % prime)) first = 0;
                else first = prime - (low_value % prime);
            }

            /*  mark every kth item  */
            for (i=first ; i<size; i+=prime) marked[i] = 1;
            
            /*  pid=0: Find next prime by finding unmarked element  */
            if (!id) { 
                while (marked[++index]);
                prime = index + 2;
            }
            MPI_Bcast(&prime, 1, MPI_INT, 0, MPI_COMM_WORLD);

        } while (prime * prime <= n);
    }

    ~~~

<br/>


<!-- LECTURE 11 -->
## <i class="fa fa-file-text-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 11

- Other MPI environment management routines
  - **```MPI_Abort(comm, errorCode)```** aborts all processes associated with the communicator ```comm```
  - **```MPI_Get_processor_name(&name, &length```** MPI version of ```gethostname``` (implementation dependent)
  - **```MPI_Initialized(&flag)```** check if ```MPI_Init``` has been called
- **<span style="color:blue">Point-to-point communication</span>**
  - MPI P2P communication has many flavors: Synchronous send, block send, non-blocking send, etc.
  - **System buffer** stores the data from multiple sends that arrive at the receiving task, which can only accpet one at a time
    - Can hurt performance because of memory copies
    - Can help performance by **allowing asynchronous send/recvs**
  - **Application buffers** are called program variables in MPI-speak
  - **<span style="color:blue">Blocking</span>**
    - Send call returns only <u>when it is safe to modify or reuse the application buffer</u>
    - Receive call returns <u>when sent data has arrived and is ready to use</u>
    - **Sync. blocking send:** send call returns when data is safely delivered to the recv process
    - **Async. blocking send:** using a send buffer to store send operations
  - **<span style="color:blue">Non-blocking</span>**
    - Non-blocking send and receive calls behave similarly and **return almost immediately**
    - Should not modify any program variable used in nonblocking communication until the operation has finished
    - **```Wait```** calls are available to test this
    - Allow **overlap of computation with communication** to achieve higher performance
  - **<span style="color:blue">Operation starvation</span>** is possible even though messages received in-order
    - For example, task<sub>2</sub> performs a single receive. task<sub>0</sub> and task<sub>3</sub> both send a message to task<sub>2</sub> that matches the receive
- **Basic Send / Recv calls and arguments**
  ~~~ c

  /* "I" forms are non-block calls */
  MPI_Send (buffer, count, type, dest,   tag, comm)
  MPI_Isend(buffer, count, type, dest,   tag, comm, request)
  MIP_Recv (buffer, count, type, source, tag, comm, status)
  MPI_Irecv(buffer, count, type, source, tag, comm, request)

  MPI_Send ()     // return after buf is free to be reused
  MPI_Recv ()     // return after the requested data is put into buf
  MPI_Ssend()     // block sender until the application buffer is free and the receiver process has started receiving the message
  MPI_Bsend()     // permit the programmer to allocate buffer space instead of system defaults
  MPI_Sendrecv()  // performs a blocking send and a blocking receive
  MPI_Buffer_attach(&buffer, size)
  MPI_Buffer_detach(&buffer, size)

  MPI_Isend()
  MPI_Irecv()
  MPI_Issend()
  MPI_Ibsend()

  MPI_Wait(&request, &status)
  MPI_Waitany(count, &array_of_requests, &index, &status)
  MPI_Waitall(count, &array_of_requests, &status)
  MPI_Waitsome(incount, &array_of_requests, &outcount, &array_of_offsets, &array_of_statuses)
  
  MPI_Probe(source, tag, comm, &status)
  MPI_Test(&request, &flag, &status)
  MPI_Testany(count, &array_of_requests, &index, &flag, &status)
  MPI_Testall(count, &array_of_requests,&flag, &array_of_statuses)
  MPI_Testsome(incount, &array_of_requests, &outcount, &array_of_offsets, &array_of_statuses)
  ~~~

  - **```buffer```**: pointer to the data to be sent or where received
  - **```count```**: number of data elements of type (not bytes) to be sent
  - **```type```**: MPI_Type
  - **```source```**: rank of the sending process in the communicator
  - **```dest```**: rank of the receiving process in the communicator
  - **```tag```**: the message type (any unsigned integer 0 - 32767)
  - **```comm```**: sender and receiver communicator
  - **```request```**: (for non-blocking) a handle to an MPI_Request structure for the operation to allow **```Wait```** type commands
  - **```status```**: pointer to the structure of type MPI_Status with fields MPI_SOURCE and MPI_TAG
  - **```MPI_ANY_SOURCE```** and **```MPI_ANY_TAG```** are the "don’t care" source and tag constants
  - **```MPI_STATUS IGNORE```** should be passed into the status field if we don’t care about the status
br/>


<!-- LECTURE 12 -->
## <i class="fa fa-file-code-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Lecture 12

- 

<br/>


<!-- ASSIGNMENT 1 -->
## <i class="fa fa-file-code-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Assignment 1

- **Goals:** Access to shared parallel resources and submit and run an OpenMP job
- **Steps:**
    1. Log in [Scholar system](https://www.rcac.purdue.edu/knowledge/scholar/accounts/login/sshkeys){:target="_blank"}
    2. Select compiler
    3. Compile
    4. Automation: generate configuration for submission 
    5. Submit jobs
    6. Check output / Rename

- **Details:**
  ~~~ shell
  
  #  [1] Log in Scholar system
  ssh-copy-id -i ~/.ssh/id_rsa.pub <USERNAME>@scholar.rcac.purdue.edu
  ssh <USERNAME>@scholar.rcac.purdue.edu

  #  [2] Select compiler
  module load intel     # intal compiler
  module load gcc       # gcc compiler
  
  #  [3] Compile
  icc -qopenmp <FILENAME>.c -o <FILENAME>    # intel compiler
  gcc -fopenmp <FILENAME>.c -o <FILENAME>    # gcc compiler

  #  [4] Automation: generate configuration for submission 
  python gen_config.py -file    <CONFIG_FILE>
                       -nodes   <INT>
                       -ntasks  <INT>
                       -time    <H:M:S> 
                       -threads <INT>
                       -code    <CODE_FILE>
                       -output  <RUN_FILE>

  #  [5] Submit jobs
  ./<RUN_FILE>

  #  [6] Check output / Rename
  cat slurm-<JOB#>.out
  mv  slurm-<JOB#>.out <USERNAME>.<JOB#>.txt

  ~~~

- **Submission configuration:**
  ~~~ shell
  
  #!/bin/bash
  # FILENAME:hw1.config
  #SBATCH --nodes=1
  #SBATCH --ntasks=20
  #SBATCH --time=00:01:00

  export OMP_NUM_THREADS=20
  ./omp_hello
  
  ~~~

- **Run file:**
  ~~~ shell
  
  sbatch hw1.config
  
  ~~~

- **Output:**
  ~~~ text

  SERIAL REGION:     Runhost:scholar-a004.rcac.purdue.edu   Thread:0 of 1 thread    hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:16 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:0 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:2 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:5 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:3 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:10 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:7 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:1 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:6 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:14 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:8 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:13 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:9 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:15 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:11 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:4 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:12 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:17 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:18 of 20 threads   hello, world
  PARALLEL REGION:   Runhost:scholar-a004.rcac.purdue.edu   Thread:19 of 20 threads   hello, world
  SERIAL REGION:     Runhost:scholar-a004.rcac.purdue.edu   Thread:0 of 1 thread    hello, world

  ~~~

<br/>


<!-- ASSIGNMENT 2 -->
## <i class="fa fa-file-code-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Assignment 2

- **Goals:** Write and time C programs in both sequential and parallel (OpenMP)
- **Tasks:**
    1.  Information of shared parallel resources (i.e., Scholar)
        ~~~ c

        /*  shared variables  */
        int id;

        /*  query the number of processors (cores)                                */
        /*  return number of possible hyperthreads on a hyperthreaded machine     */
        /*  (i.e., the MAX number of threads scheduled to run at any given time)  */
        printf("number of cores: %d\n", omp_get_num_procs());

        #pragma omp parallel
        {
            /*  (A.1) threadId for each thread  */
            id = omp_get_thread_num();
            printf("threadId in parallel: %d\n", id);

            /*  (A.2) single  */
            #pragma omp single 
            printf("threadId in single: %d\n", omp_get_thread_num());

            /*  (A.3) master  */
            #pragma omp master 
            printf("threadId in master: %d\n", omp_get_thread_num());
        }
       
        ~~~

        ~~~ text

        number of cores: 20
        threadId in parallel: 17
        threadId in parallel: 11
        threadId in parallel: 8
        threadId in parallel: 15
        threadId in parallel: 13
        threadId in single: 8
        threadId in parallel: 10
        threadId in parallel: 14
        threadId in parallel: 7
        threadId in parallel: 4
        threadId in parallel: 12
        threadId in parallel: 6
        threadId in parallel: 1
        threadId in parallel: 3
        threadId in parallel: 18
        threadId in parallel: 9
        threadId in parallel: 2
        threadId in parallel: 5
        threadId in parallel: 16
        threadId in parallel: 0
        threadId in parallel: 19
        threadId in master: 0

        ~~~
        <br/>
    2.  Sequential vs. Parallel
        ~~~ c

        /*  B-1.
            Perform a sequential sum reduction on the array and time and print the execution time  */
        int N = 1000000;
        double array[N];

        int i;

        //  [+] execution time
        double START = omp_get_wtime();

        //  (1) initialization
        for (i=0 ; i<N; i++) { array[i] = 1.0; }

        //  (2) sum
        double ans = 0;    
        for (i=0 ; i<N; i++) { ans += array[i]; }

        //  [+] execution time
        double END = omp_get_wtime();

        double msec = (END - START) * 1000;
        printf("\n[ Sequential Sum Reduction ]\n");
        printf("total:          %f\n", ans);
        printf("execution time: %lf milliseconds (ms)\n", msec);


        /*  B-2.
            Perform a parallel sum reduction on the array and time and print the execution time  */
        int nt = omp_get_max_threads();
        double ans_array[nt*8];   // to reduce this ping-ponging effect

        //  [+] execution time
        START = omp_get_wtime();

        //  (1) initialization
        for (i=0 ; i<N; i++) { array[i] = 1.0; }
        for (i=0 ; i<nt*8; i++) { ans_array[i] = 0.0; }

        //  (2) sum
        #pragma omp parallel for
        for (i=0 ; i<N; i++) { ans_array[omp_get_thread_num()*8] += array[i]; }
        for (i=1 ; i<nt; i++) { ans_array[0] += ans_array[i*8]; }

        //  [+] execution time
        END = omp_get_wtime();

        msec = (END - START) * 1000;
        printf("\n[ Parallel Sum Reduction ]\n");
        printf("threads:        %d\n", nt);
        printf("total:          %f\n", ans_array[0]);
        printf("execution time: %lf milliseconds (ms)\n", msec);


        /*  B-3.
            Perform an OpenMP sum reduction on the array and time and print the execution time  */
        //  [+] execution time
        START = omp_get_wtime();

        //  (1) initialization
        for (i=0 ; i<N; i++) { array[i] = 1.0; }

        //  (2) sum
        ans = 0;
        #pragma omp parallel for reduction(+:ans)
        for (i=0 ; i<N; i++) { ans += array[i]; }

        //  [+] execution time
        END = omp_get_wtime();

        msec = (END - START) * 1000;
        printf("\n[ OpenMP Sum Reduction ]\n");
        printf("total:          %f\n", ans);
        printf("execution time: %lf milliseconds (ms)\n\n", msec);
       
        ~~~

        ~~~ text

        [ Sequential Sum Reduction ]
        total:          1000000.000000
        execution time: 2.757072 milliseconds (ms)

        [ Parallel Sum Reduction ]
        threads:        20
        total:          1000000.000000
        execution time: 2.767086 milliseconds (ms)

        [ OpenMP Sum Reduction ]
        total:          1000000.000000
        execution time: 0.997066 milliseconds (ms)
       
        ~~~
        <br/>
    3.  Floating point overflow
        ~~~ c

        /*  query the number of processors (cores)                                */
        /*  return number of possible hyperthreads on a hyperthreaded machine     */
        /*  (i.e., the MAX number of threads scheduled to run at any given time)  */
        printf("number of cores: %d\n\n", omp_get_num_procs());

        float ans1 = 0;
        float ans2 = 0;
        float ans3 = 0;

        int i;

        /*  (C.1) sequential loop 1  */
        for(i=1 ; i<10000000 ; i++) {
            ans1 += 1.0 / i;
        }

        /*  (C.2) sequential loop 2  */
        for(i=10000000 ; i>0 ; i--) {
            ans2 += 1.0 / i;
        }

        /*  (C.3) parallel loop  */
        #pragma omp parallel for reduction(+:ans3)
        for(i=1 ; i<10000000 ; i++) {
            ans3 += 1.0 / i;
        }

        printf("[float]\n");
        printf("sum of first  loop: %f\n", ans1);
        printf("sum of second loop: %f\n", ans2);
        printf("sum of third  loop: %f\n", ans3);

        /*  use double  */
        double ans4 = 0;
        double ans5 = 0;
        double ans6 = 0;

        /*  (C.1) sequential loop 1  */
        for(i=1 ; i<10000000 ; i++) {
            ans4 += 1.0 / i;
        }

        /*  (C.2) sequential loop 2  */
        for(i=10000000 ; i>0 ; i--) {
            ans5 += 1.0 / i;
        }

        /*  (C.3) parallel loop  */
        #pragma omp parallel for reduction(+:ans6)
        for(i=1 ; i<10000000 ; i++) {
            ans6 += 1.0 / i;
        }

        printf("\n[double]\n");
        printf("sum of first  loop: %f\n", ans4);
        printf("sum of second loop: %f\n", ans5);
        printf("sum of third  loop: %f\n", ans6);
       
        ~~~

        ~~~ text

        number of cores: 20

        [float]
        sum of first  loop: 16.693398
        sum of second loop: 16.695377
        sum of third  loop: 16.695314

        [double]
        sum of first  loop: 16.695311
        sum of second loop: 16.695311
        sum of third  loop: 16.695311
       
        ~~~
        <br/>
    4.  Size of cache line, Tiling
        ~~~ c

        #include <time.h>
        #include <stdlib.h>
        #include <cstdio>

        /*  VERY SLOW  */

        #define NUMROWS 2520
        #define NUMCOLS 2520
        #define idx(u, y, x) (u[y*NUMCOLS + x])

        float* newArray(int rows, int cols) {
          float* a = (float*) malloc(NUMROWS * NUMCOLS * sizeof(float)); 
          for (int i = 0; i < cols; i++) {
              for (int j = 0; j < cols; j++) {
                idx(a,i,j) = 1.0;
              }
          }
          return a;
        }

        int main(int argc, char** args) {

          float* a = newArray(NUMROWS, NUMCOLS);
          float* b = newArray(NUMROWS, NUMCOLS);
          float* c = newArray(NUMROWS, NUMCOLS);

          clock_t begin = clock();
          for (int i = 0; i < NUMROWS; i++) {
              for (int j = 0; j < NUMCOLS; j++) {
                float comp = 0.;
                for (int k = 0; k < NUMCOLS; k++) {
                    comp += idx(a,i,k)*idx(b,k,j);
                }
                idx(c,i,j) = comp;
              }
          }
          clock_t end = clock();
          double time_spent = (double)(end - begin) / CLOCKS_PER_SEC;
          printf("Elapsed: %f seconds\n", time_spent);
        }
       
        ~~~

        ~~~ c

        #include <time.h>
        #include <stdlib.h>
        #include <cstdio>

        /*  SLOW  */

        #define NUMROWS 2520
        #define NUMCOLS 2520
        #define TILESIZE 90
        #define idx(u, y, x) (u[y*NUMCOLS + x])

        float* newArray(int rows, int cols) {
          float* a = (float*) malloc(NUMROWS * NUMCOLS * sizeof(float)); 
          for (int i = 0; i < cols; i++) {
              for (int j = 0; j < cols; j++) {
                idx(a,i,j) = 1.0;
              }
          }
          return a;
        }

        int main(int argc, char** args) {

          float* a = newArray(NUMROWS, NUMCOLS);
          float* b = newArray(NUMROWS, NUMCOLS);
          float* c = newArray(NUMROWS, NUMCOLS);

          clock_t begin = clock();
          for (int tI = 0; tI < NUMROWS; tI += TILESIZE) {
              for (int tJ = 0; tJ < NUMCOLS; tJ += TILESIZE) {
                for (int i = tI; i < tI+TILESIZE; i++) {
                    for (int j = tJ; j < tJ+TILESIZE; j++) {
                      float comp = 0.;
                          for (int k = 0; k < NUMCOLS; k++) {
                            comp += idx(a,i,k)*idx(b,k,j);
                          }
                      idx(c,i,j) = comp;
                    }
                }
              }
          }
          clock_t end = clock();
          double time_spent = (double)(end - begin) / CLOCKS_PER_SEC;
          printf("Elapsed: %f seconds\n", time_spent);
        }

        ~~~
<br/>


<!-- ASSIGNMENT 3 -->
## <i class="fa fa-file-code-o" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Assignment 3

- **Goals:** OpenMP practice
- **Tasks:**
    1.  Private variable of OpenMP parallelization
        ~~~ c

        int i;


        /*  Method 1  */
        int tid;
        #pragma omp parallel for private(tid) 
        for(i=0 ; i<omp_get_num_threads() ; i++) {
          tid = omp_get_thread_num();

          #pragma omp critical
          printf("tid: %d, tid address: %p\n",tid, (void*) &tid); 
        }


        /*  Method 2  */
        #pragma omp parallel for 
        for(i=0 ; i<omp_get_num_threads() ; i++) {
          int tid = omp_get_thread_num();

          #pragma omp critical
          printf("tid: %d, tid address: %p\n",tid, (void*) &tid); 
        }


        /*  Method 3  */
        #pragma omp parallel for 
        for(i=0 ; i<omp_get_num_threads() ; i++) {
          tid = omp_get_thread_num();

          #pragma omp critical
          printf("tid: %d, tid address: %p\n",tid, (void*) &tid); 
        }

        ~~~
        <br/>
    2.  Critical vs. Master / Single
        ~~~ c

        #pragma omp parallel
        #pragma omp critical
        printf("Thread ID (critical): %d\n", omp_get_thread_num());

        #pragma omp parallel
        #pragma omp master
        printf("Thread ID (master):   %d\n", omp_get_thread_num());

        // #pragma omp parallel
        #pragma omp single
        printf("Thread ID (single):   %d\n", omp_get_thread_num());
       
        ~~~

        ~~~ text

        Thread ID (critical): 4
        Thread ID (critical): 3
        Thread ID (critical): 0
        Thread ID (critical): 2
        Thread ID (critical): 1
        Thread ID (master):   0
        Thread ID (single):   0
       
        ~~~
        <br/>
    3.  Parallel sections
        ~~~ c

        int N = 1000000;
        double array1[N/2], array2[N];
        double sum1, sum2;
        
        int i;
        for(i=0 ; i<N/2; i++) { array1[i] = 1.0; }
        for(i=0 ; i<N;   i++) { array2[i] = 1.0; }
        
        #pragma omp parallel sections
        {
            #pragma omp section
            {
                sum1 = sum(array1, N/2);
                printf("[tid %d] sum of array1 = %f\n", omp_get_thread_num(), sum1);
            }
            
            #pragma omp section
            {
                sum2 = sum(array2, N);
                printf("[tid %d] sum of array2 = %f\n", omp_get_thread_num(), sum2);
            }
        }
       
        ~~~

        ~~~ text

        [tid 0] sum of array1 = 500000.000000
        [tid 1] sum of array2 = 1000000.000000

        ~~~
        <br/>
    4.  Size of cache line, Tiling
        ~~~ c

        int i;

        #pragma omp parallel private(i)
        for(i=0 ; i<3 ; i++) { printf("forLoop after omp parallel:     tid = %3d, iter = %3d\n", omp_get_thread_num(), i); }

        printf("\n");

        #pragma omp parallel for
        for(i=0 ; i<3 ; i++) { printf("forLoop after omp parallel for: tid = %3d, iter = %3d\n", omp_get_thread_num(), i); }
       
        ~~~

        ~~~ text

        forLoop after omp parallel:     tid =   2, iter =   0
        forLoop after omp parallel:     tid =   4, iter =   0
        forLoop after omp parallel:     tid =   0, iter =   0
        forLoop after omp parallel:     tid =   0, iter =   1
        forLoop after omp parallel:     tid =   0, iter =   2
        forLoop after omp parallel:     tid =   3, iter =   0
        forLoop after omp parallel:     tid =   3, iter =   1
        forLoop after omp parallel:     tid =   1, iter =   0
        forLoop after omp parallel:     tid =   3, iter =   2
        forLoop after omp parallel:     tid =   1, iter =   1
        forLoop after omp parallel:     tid =   1, iter =   2
        forLoop after omp parallel:     tid =   2, iter =   1
        forLoop after omp parallel:     tid =   4, iter =   1
        forLoop after omp parallel:     tid =   4, iter =   2
        forLoop after omp parallel:     tid =   2, iter =   2

        forLoop after omp parallel for: tid =   2, iter =   2
        forLoop after omp parallel for: tid =   0, iter =   0
        forLoop after omp parallel for: tid =   1, iter =   1

        ~~~
<br/>


<!-- PROJECT -->
## <i class="fa fa-file-text" aria-hidden="true" style="font-size:75%;margin-right:5px;"></i> Project

<figure>
	<img src="/assets/img/ece-563-1.png" style="border:2px solid #555;border-radius:6px">
	<figcaption style="text-align:center">Program architecture</figcaption>
</figure>

- **Goals:** Map-reduce + Word count problems

<br/>

> **Author:** Yuan-Yao Lou / Ph.D. student, ECE, Purdue<br/>
  <small>(Last update: Sep 05, 2022)</small><br/>